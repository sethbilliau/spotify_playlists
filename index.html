<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>CS 109a Project</title>
    <link rel="stylesheet" href="css/bootstrap.min.css">
    <link rel="stylesheet" type="text/css" href="css/jquery.pagepiling.min.css">
    <link rel="stylesheet" href="css/style.css">
    <link rel="icon" href="images/spotify_icon.png">
</head>
<body>

<!-- Top nav bar -->
    <nav class="navbar navbar-expand-lg navbar-light bg-light navbar-custom">
        <a class="navbar-brand" href="#">CS 109a Project</a>
        <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
            <span class="navbar-toggler-icon"></span>
        </button>

        <div class="collapse navbar-collapse" id="navbarSupportedContent">
            <ul class="navbar-nav mr-auto" id="myMenu">
                <li data-menuanchor="firstPage" class="nav-item active">
                    <a class="nav-link" href="#firstPage">Home</a>
                </li>
                <li data-menuanchor="secondPage" class="nav-item">
                    <a class="nav-link" href="#secondPage">Abstract</a>
                </li>
                <li data-menuanchor="thirdPage" class="nav-item">
                    <a class="nav-link" href="#thirdPage">Literature Review</a>
                </li>
                <li data-menuanchor="fourthPage" class="nav-item">
                    <a class="nav-link" href="#fourthPage">Data and EDA</a>
                </li>
                <li data-menuanchor="fifthPage" class="nav-item">
                    <a class="nav-link" href="#fifthPage">Methods and Models</a>
                </li>
                <li data-menuanchor="sixthPage" class="nav-item">
                    <a class="nav-link" href="#sixthPage">Results</a>
                </li>
                <li data-menuanchor="seventhPage" class="nav-item">
                    <a class="nav-link" href="#seventhPage">Conclusions</a>
                </li>
                <li data-menuanchor="eighthPage" class="nav-item">
                    <a class="nav-link" href="#eighthPage">Bibliography</a>
                </li>
            </ul>
        </div>
    </nav>

<!-- Page piling, to create a new page, add a new div with class "section" -->
    <div id="pagepiling">
        <div class="section pp-scrollable">
            <img src="images/spotify_logo.png" alt="spotify logo" class="center" width = "800">

            <br><br><br>
            <h1 style="text-align: center;">Exploring the Cold Start Problem via Spotify</h1>
            <br>
            <div style="text-align: center;">
                <p><b>Team members:</b> Seth Billiau, William Drew, Sarah Lucioni</p>
                <p><b>Project TF:</b> Nathan Hollenberg</p>
                <p><b>Last edited:</b> 12/11/2019</p>
                <p><b>Group ID:</b> 56</p>
            </div>

            <br>
            <div class="outline-div">
                <h5>Project Outline</h5>
                <ul class="no-blt-list">
                    <li><a href="#secondPage">Abstract</a></li>
                    <ul class="no-blt-list">
                        <li>Motivation and Project Questions</li>
                        <li>Organization of the Report</li>
                        <li>Works Cited</li>
                    </ul>

                    <li><a href="#thirdPage">Literature Review</a></li>
                    <ul class="no-blt-list">
                        <li>Spotify's Recommendation System</li>
                        <ul class="no-blt-list">
                            <li>Collaborative Filtering</li>
                            <li>Natural Language Processing</li>
                            <li>Audio Modeling</li>
                            <li>Works Cited</li>
                        </ul>
                    </ul>

                    <li><a href="#fourthPage">Data and EDA</a></li>
                    <ul class="no-blt-list">
                        <li>Data Sources and Features</li>
                        <li>EDA</li>
                    </ul>

                    <li><a href="#fifthPage">Methods and Models</a></li>
                    <ul class="no-blt-list">
                        <li>Model Type and Description</li>
                        <li>Data</li>
                        <li>Response and Predictor Variables</li>
                        <li>Model</li>
                        <ul class="no-blt-list">
                            <li>k-Nearest Neighbors</li>
                            <li>k-Means Clustering</li>
                            <li>k-Nearest Neighbors vs. k-Means Clustering</li>
                        </ul>
                        <li>Final Models</li>
                        <li>Works Cited</li>
                    </ul>

                    <li><a href="#sixthPage">Results: Evaluating our Predicted Playlists</a></li>
                    <ul class="no-blt-list">
                        <li>Deviation from Seed Song Audio Features</li>
                        <li>Selecting Seed Songs</li>
                        <li>Results</li>
                        <li>Works Cited</li>
                    </ul>

                    <li><a href="#seventhPage">Conclusions</a></li>

                    <li><a href="#eighthPage">Bibliography</a></li>
                </ul>
            </div>
        </div>

        <div class="section pp-scrollable">
            <h1>Abstract</h1>
            <h3>Motivation and Project Questions</h3>
            <p>When it comes to music, streaming is the way of the future. Convenient, cheap, and widely accessible, streaming is easier than ever with more people turning to popular services for their musical needs. In April of 2019, industry leader Spotify announced that they had reached a staggering 217 million monthly active users with over 100 million paid subscribers (Porter, “Spotify Is First to 100 Million Paid Subscribers.”).</p>

            <p>In addition to providing music for its users, Spotify also provides a robust music recommendation system. Leveraging data science and machine learning techniques, Spotify aggregates a number of different models to generate new playlists based on a user’s musical preferences.</p>

            <p>Developing a user’s musical taste is no small feat, but Spotify is able to overcome this hurdle by collecting user feedback on generated playlists with an up/down vote system and by aggregating information from a user’s listening history. But what if they didn’t have all of that information?  How should Spotify generate a new playlist for a brand new user given minimal user information? Imagine a new user stumbles upon Spotify for the first time and plays a song. From this single song, is it possible to generate a likeable playlist?</p>

            <p>Our project attempts to answer this question by developing such a "cold start" model, generating a new playlist for a user given just a single song. </p>

            <h3>Organization of the Report</h3>
            <p>After a brief literature review, this report will begin with an exploration of the dataset and its features. We will continue by explaining our methods, models, and forms of evaluation. The final sections of our project will present our results and conclusions with a brief discussion of possible extensions to our project.</p>


            <h5>Works Cited</h5>
              <ol>
                 <li>
                    <p>Porter, “Spotify Is First to 100 Million Paid Subscribers.”</p>
                 </li>
                 <li>
                    <p>“Discover Weekly: How Spotify Is Changing the Way We Consume Music – Technology and Operations Management.”</p>
                 </li>
              </ol>
        </div>
         <div class="section pp-scrollable">
            <h1>Literature Review</h1>
            <p>Before diving into our project, we reviewed various sources of relevant literature, researching different tools that could help us accomplish our project goal. First, we learned as much as we could about Spotify’s internal recommendation system.<p>

            <h3>Spotify’s Recommendation System</h3>
            <p>Spotify’s music recommendation system uses a combination of three different models when creating new playlists for existing users: Collaborative filtering, Natural Language Processing (NLP), and Audio modeling. These three methods are described below.</p>

            <h5>Collaborative Filtering</h5>
            <p>Collaborative filtering is an algorithm that predicts a user’s interests based on the interests of similar peers. Spotify leverages this technique by using its wealth of customer listening data, identifying similar individuals to a target customer and creating a new playlist with songs enjoyed by those similar individuals. For example, say Spotify knows that target user X has listened to songs 3, 4, 9, and 10 while user Y has listened to songs 1, 2, 9, and 10. Using collaborative filtering, Spotify might add songs 1 and 2 to a new playlist for user X (Johnson, “Algorithmic Music Recommendations at Spotify.”).</p>

            <h5>Natural Language Processing (NLP)</h5>
            <p>Spotify analyzes text from popular blog posts, internet data, and music publications to see which songs are being commonly discussed and if songs are frequently being discussed together. Using this text analysis, Spotify attempts to cluster descriptions and adjectives to get a sense of what people think of a given song or artist. Based on this information and past user information, Spotify will attempt to predicts new songs for a target user. For example, say target user X has listened to song 1. Based on NLP information, Spotify learns that song 1 is often discussed with song 2 and is often described as “dancey”. Therefore, Spotify might suggest song 2 in a new playlist for user X along with other songs that have been identified as “dancey” (Giacaglia, “Behind Spotify Recommendation Engine.”).</p>

            <h5>Audio modeling</h5>
            <p>Using a Convolutional Neural Network, Spotify analyzes the input raw audio of every song on the service, producing comparable characteristics like time signature, key, tempo, loudness, etc., to compare songs. Therefore, if Spotify knows a target user has listening to song 1 with certain audio features, it will suggest songs with similar audio features in a new playlist. These audio features can be accessed via the Spotify developer tools (Dieleman. “Recommending Music on Spotify with Deep Learning.”).</p>

            <p>Based on this research, we decided to explore an approach similar to Spotify’s audio modeling techniques in our project. With only a single input song, we do not have enough information on our base user to use collaborative filtering in an effective way. We also don’t have the bandwidth or capabilities to perform NLP with the tools we have learned in CS109. However, Spotify’s song audio features provide easily-comparable metrics by which to identify similar songs given one input track.</p>


            <h5>Works Cited</h5>
              <ol>
                 <li>
                    <p>Chris Johnson. “Algorithmic Music Recommendations at Spotify.” Technology, 11:33:57 UTC. <br> https://www.slideshare.net/MrChrisJohnson/algorithmic-music-recommendations-at-spotify/3-What_is_SpotifyOn_demand_music.</p>
                 </li>
                 <li>
                    <p>Giacaglia, Giuliano. “Behind Spotify Recommendation Engine.” Medium, May 21, 2019. <br> https://medium.com/datadriveninvestor/behind-spotify-recommendation-engine-a9b5a27a935.</p>
                 </li>
                 <li>
                     <p>Sander Dieleman. “Recommending Music on Spotify with Deep Learning.” Accessed December 9, 2019. <br>https://benanne.github.io/2014/08/05/spotify-cnns.html.</p>
                 </li>
              </ol>
        </div>

        <div class="section pp-scrollable">
            <h1>Data and EDA</h1>
            <p>Given our literature research, the goal of our data cleaning was to obtain Spotify audio features to be used for modeling in our project. This section describes that process.</p>
            <h3>Data Sources and Features</h3>
            <p>This project uses the Spotify Songs dataset, a collection of one million playlists organized into 1000 .csv files. Each row of these .csv files is a song object with the variable <i>pid</i> denoting the playlist ID and linking the corresponding song objects to the playlist. The features of our data are as follows: </p>

            <li><i>pid:</i> the song’s corresponding playlist ID </li>
            <li><i>pos:</i> the song’s position on the playlist</li>
            <li><i>artist_name:</i> the song’s artist name</li>
            <li><i>track_uri:</i> the Uniform Resource Indicator (URI) link for the song (which we can query the Spotify API with)</li>
            <li><i>artist_uri:</i>  the Uniform Resource Indicator link for the artist</li>
            <li><i>track_name:</i> the name of the song</li>
            <li><i>album_uri:</i> the Uniform Resource Indicator link for the album the song is on </li>
            <li><i>duration_ms:</i> the length of the song in ms</li>
            <li><i>album_name:</i> the name of the album the song is on</li>

            <br/>

            <p>We began by performing some preliminary analysis to explore how we can engineer these given features to our advantage. Upon an initial inspection, we saw that we needed to clean three variables: <i>track_uri, artist_uri</i>, and <i>album_uri</i>. Originally, all three of these URI's showed the header <i>“spotify:track/artist/album:URI”</i>. However, the useful part of each of these features was the URI code that followed this header since it can be used to query the Spotify API. Therefore, we wrote a function to strip off this header and reformatted these features to include only the useful parts of the URI.</p>

            <p>Using these URI’s we queried the Spotify API using the Spotipy library to access the API with Python, extracting the audio features for each song in our dataset. The audio features we plan to examine most closely are:</p>

            <li><i>acousticness:</i> [0, 1] value determining whether track is acoustic</li>
            <li><i>danceability:</i> [0, 1] value determining whether track is danceable</li>
            <li><i>duration:</i> track duration in ms</li>
            <li><i>energy:</i> [0, 1] value determining relative energy of track, typically classical to heavy metal, soft to intense/loud/full of activity </li>
            <li><i>instrumentalness:</i> [0, 1] value determining whether track has vocal content, closer to 1 is closer to instrumental </li>
            <li><i>liveness:</i> [0, 1] value determining whether track is recorded in front of a live audience</li>
            <li><i>loudness:</i> how loud the song is in decibels (dB). Typically -60 to 0 db</li>
            <li><i>mode:</i> [0, 1] value determining whether track is more minor or major</li>
            <li><i>speechiness:</i> [0, 1] value determining spoken words in track (below 0.33 likely represents music, non-spoken tracks) </li>
            <li><i>tempo:</i> song's tempo in beats per minute (BPM) </li>
            <br/>

            <p>When parsing the playlists, we ran into some problems where occasionally the Spotify API did not return a complete audio feature list when provided with specific song URIs. This prevented the entire playlist from being parsed correctly, so we handled this problem by skipping these playlists whenever we ran into this issue. We do not believe that this has any significant impact on the collection of songs we compiled as we could not identify any systematic pattern nor commonality between the problematic playlists. </p>

            <p>From our dataset of one million playlists, we parsed 30,000 of them and collected audio features for nearly 200,000 songs. We stopped at parsing through only 30,000 playlists due to technical limitations and time constraints. By our estimation, it would have taken us multiple days and memory usage beyond the capabilities of our laptops to parse through all one million playlists. Of course, by limiting the number of playlists we parsed, we are limiting the size of our music library that we can recommend. We decided that the first 30,000 playlists mapping to 200,000 songs provided a good enough distribution of music genres, types, and styles to provide interesting and novel recommendations to our users. </p>

            <p> </p>
            <br/>
            <h3>EDA</h3>
            <p>After successfully extracting audio features for each song in our dataset, we began by visualizing the distribution of these audio features.

            We then wrote a function to extract a list of the tracks in a playlist and another function to return summary statistics about the playlist’s audio features. The latter function uses the Spotify API to extract the audio features of each song in the playlist. Based on the audio features, we further explored the data by creating visualizations which are displayed in the following section. We also created visualizations based on the number of songs in a playlist and the duration (in minutes) of a playlist.</p>

            <p>We looked at all nine of the music features mentioned above as possible parameters for our song suggestion model. To look at the general distribution of features for our parsed songs, we constructed boxplots for each feature as well as obtained summary statistics for each.  </p>

            <img class="center-img" src="images/features1.png" width="80%">
            <img class="center-img" src="images/features2.png" width="80%">

            <br>
            <table class="tg">
              <tr>
                <th class="tg-0pky row-col-title"></th>
                <th class="tg-0pky row-col-title">Acousticness</th>
                <th class="tg-0pky row-col-title">Danceability</th>
                <th class="tg-0pky row-col-title">Duration_ms</th>
                <th class="tg-0pky row-col-title">Energy</th>
                <th class="tg-0pky row-col-title">Instrumentalness</th>
                <th class="tg-0pky row-col-title">Liveness</th>
                <th class="tg-0pky row-col-title">Loudness</th>
                <th class="tg-0pky row-col-title">Speechiness</th>
                <th class="tg-0pky row-col-title">Tempo</th>
              </tr>
              <tr>
                <td class="tg-0pky row-col-title">Mean</td>
                <td class="tg-0pky">0.324306</td>
                <td class="tg-0pky">0.556000</td>
                <td class="tg-0pky">244180.2</td>
                <td class="tg-0pky">0.603160</td>
                <td class="tg-0pky">0.181495</td>
                <td class="tg-0pky">0.208895</td>
                <td class="tg-0pky">-8.999104</td>
                <td class="tg-0pky">0.088199</td>
                <td class="tg-0pky">120.487872</td>
              </tr>
              <tr>
                <td class="tg-0pky row-col-title">StD</td>
                <td class="tg-0pky">0.340081</td>
                <td class="tg-0pky">0.180091</td>
                <td class="tg-0pky">128201.5</td>
                <td class="tg-0pky">0.256615</td>
                <td class="tg-0pky">0.323783</td>
                <td class="tg-0pky">0.188702</td>
                <td class="tg-0pky">5.159350</td>
                <td class="tg-0pky">0.107022</td>
                <td class="tg-0pky">29.685628</td>
              </tr>
              <tr>
                <td class="tg-0pky row-col-title">Min</td>
                <td class="tg-0pky">0.000000</td>
                <td class="tg-0pky">0.000000</td>
                <td class="tg-0pky">305600</td>
                <td class="tg-0pky">0.000000</td>
                <td class="tg-0pky">0.000000</td>
                <td class="tg-0pky">0.000000</td>
                <td class="tg-0pky">-60.000000</td>
                <td class="tg-0pky">0.000000</td>
                <td class="tg-0pky">0.000000</td>
              </tr>
              <tr>
                <td class="tg-0pky row-col-title">Max</td>
                <td class="tg-0pky">0.996000</td>
                <td class="tg-0pky">0.436000</td>
                <td class="tg-0pky">527976.8</td>
                <td class="tg-0pky">1.000000</td>
                <td class="tg-0pky">1.000000</td>
                <td class="tg-0pky">1.000000</td>
                <td class="tg-0pky">3.744000</td>
                <td class="tg-0pky">0.967000</td>
                <td class="tg-0pky">247.963000</td>
              </tr>
              <tr>
                <td class="tg-0pky row-col-title">25%</td>
                <td class="tg-0pky">0.019700</td>
                <td class="tg-0pky">0.569000</td>
                <td class="tg-0pky">188352.8</td>
                <td class="tg-0pky">0.422000</td>
                <td class="tg-0pky">0.000000</td>
                <td class="tg-0pky">0.096300</td>
                <td class="tg-0pky">-11.030000</td>
                <td class="tg-0pky">0.035100</td>
                <td class="tg-0pky">97.410000</td>
              </tr>
              <tr>
                <td class="tg-0pky row-col-title">Median</td>
                <td class="tg-0pky">0.176000</td>
                <td class="tg-0pky">0.691000</td>
                <td class="tg-0pky">225801.5</td>
                <td class="tg-0pky">0.642000</td>
                <td class="tg-0pky">0.000207</td>
                <td class="tg-0pky">0.128000</td>
                <td class="tg-0pky">-7.659000</td>
                <td class="tg-0pky">0.047000</td>
                <td class="tg-0pky">120.025000</td>
              </tr>
              <tr>
                <td class="tg-0pky row-col-title">75%</td>
                <td class="tg-0pky">0.617000</td>
                <td class="tg-0pky">0.990000</td>
                <td class="tg-0pky">274297</td>
                <td class="tg-0pky">0.818000</td>
                <td class="tg-0pky">0.163000</td>
                <td class="tg-0pky">0.264000</td>
                <td class="tg-0pky">-5.528000</td>
                <td class="tg-0pky">0.086400</td>
                <td class="tg-0pky">138.976250</td>
              </tr>
            </table>

            <br>
            <p>From these boxplots, we see that the distributions for acousticness, dancability, and energy are relatively wide. This indicates that the songs in our dataset are especially varied with respect to these three parameters, indicating three parameters that could be significant when developing our model to generate song suggestions. The other parameters - instrumentalness, liveness, and speechiness - do not have as wide of a distribution, indicating that it could be difficult to differentiate songs from one another using these features. We observed similar trends for duration, tempo, and loudness. </p>

            <p>We also looked at lengths of playlists in our parsed dataset - both in number of songs and total duration in minutes. We found that playlists tended to have around 50 songs as a good generalization and tended to have a total running time of roughly 4 hours. This gives us a good idea for how long our suggested playlists should be.  </p>

            <div class="two-imgs">
                <img class="center-img" src="images/songcount.png" width="45%">
                <img class="center-img" src="images/duration.png" width="45%">
            </div>

            <br>
            <table class="tg">
              <tr>
                <th class="tg-0pky row-col-title"></th>
                <th class="tg-0pky row-col-title">Playlist Song Count</th>
                <th class="tg-0pky row-col-title">Playlist Duration (min)</th>
              </tr>
              <tr>
                <td class="tg-0pky row-col-title">Mean</td>
                <td class="tg-0pky">66.346</td>
                <td class="tg-0pky">259.661</td>
              </tr>
              <tr>
                <td class="tg-0pky row-col-title">StD</td>
                <td class="tg-0pky">53.669</td>
                <td class="tg-0pky">214.273</td>
              </tr>
              <tr>
                <td class="tg-0pky row-col-title">Median</td>
                <td class="tg-0pky">49.0</td>
                <td class="tg-0pky">190.373</td>
              </tr>
              <tr>
                <td class="tg-0pky row-col-title">Max</td>
                <td class="tg-0pky">376</td>
                <td class="tg-0pky">10584.563</td>
              </tr>
              <tr>
                <td class="tg-0pky row-col-title">Min</td>
                <td class="tg-0pky">5</td>
                <td class="tg-0pky">1.625</td>
              </tr>
            </table>
            <br><br><br><br>
            <br/>

        </div>

        <div class="section pp-scrollable">
            <h1>Methods and Models</h1>
            <p>To address our question of how to generate a new playlist for a new user given minimal user information, we first considered the data we would use, the transformations we would need to apply, the output we wanted, and the input needed to predict the output before deciding on a model type. Rewording our problem statement, our model must predict a playlist given minimal user information.</p>

            <h3>Data</h3>
            <p>The data is unconventional because our data of playlists of songs are technically categorical data. However, each unique song is it’s own “category” which makes traditional categorical transformations such as one-hot encoding uninterpretable. Using the Spotify API, we can extract audio features for each song (see Data and EDA). The audio features transform each song to a set of numerical features better suited for traditional prediction techniques. We selected the audio features acousticness, danceability, energy, instrumentalness, liveness, speechiness, and tempo to analyze. We chose these seven features because they are rarely omitted from the Spotify audio features data (an omitted audio feature appears as a 0 in the Spotify data) and they are all normalized values (except tempo). Tempo not being normalized led to several model variations which we will discuss below.</p>
            <p>As discussed in the Data and EDA section, our dataset contains one million playlists. To reduce the computational power needed and the Spotify API calls made, we decided to train our model on the first 30,000 playlists and test on the next 1000 playlists. This train - test split is valid because the data is already randomly sorted. Therefore, both the train and test sets should reflect the overall data.</p>

            <h3>Response and Predictor Variables</h3>
            <p>The output is an n-length playlist of n-songs. This means that our model should predict n-songs. To do so, our transformed song data (to audio features) should also keep a reference back to it’s song. This reference will be used to display the predicted playlist. </p>
            <p>The input needs to be a datum that our model can predict songs based off of, but it must be minimal to match our problem statement. Due to these constraints, we decided that our input would be a “seed song” as well as a playlist length. The seed song will be in the form of a Spotify URI so that we can extract the audio features from it. We also input a playlist length to simplify the predictive tasks of the model. However, a separate model could be written to predict the length of the playlist, but we did not find this necessary to implement to address our problem statement. <p>
            <p>Therefore, our model must predict an n-length playlist of n-songs given n and a seed song.<p>

            <h3>Model</h3>
            <p>With one seed song, a natural approach suggests creating a playlist from the n closest songs to the seed. This motivates our first set of models: <b>k-Nearest Neighbors</b> based. Our second set of models is a logical comparison to the first: <b>k-Means Clustering</b> based. Let us first define each type of model, then discuss the differences, and finally, present our constructed models.<p>

            <h5>k-Nearest Neighbors</h5>
            <p>k-NN models are supervised machine learning algorithms used for classification or regression. Typically, the k nearest neighbors are accessed to predict the classification (most frequently occurring class in the k) or regression (mean of the k nearest neighbors). The k-NN algorithm is an instance of lazy learning which means that the computation is deferred until it must output a prediction (“Machine Learning Basics with the K-Nearest Neighbors Algorithm”). One variation of a k-NN model is the choice of distance metric. A proper distance metric must satisfy three properties:</p>

            <ol>
              <li>d(x, x) = 0, The distance between the same point equals 0.</li>
              <li>d(x, x’) = d(x’, x), The distance function must be symmetric.</li>
              <li>d(x, x’’) = d(x, x’) + d(x’, x’’), The triangle inequality must hold.</li>
            </ol>

            <p>Euclidean distance is commonly the selected distance metric for k-NN. However, cosine similarity may be effective in some cases. Cosine similarity does not respect the above properties, but may be used as a distance metric if appropriately treated within the model. The choice of distance metric presents the first variation in our set of k-NN models. We create one set of models using euclidean distance and a second set using cosine similarity.</p>
            <p>Due to the distance metric, k-NN typically performs best when the data is all on the same scale (Dhiraj, “Difference between K-Nearest Neighbor(K-NN) and K-Means Clustering”). This limitation raises a concern about training our k-NN models on our data including tempo. The other six features are on a [0, 1] scale. However, tempo varies greatly and is hard to normalize because there is not a common base or top value. This presents a second variation in our set of k-NN models: include vs. omit tempo from the training data. </p>
            <p>In our set of k-NN based models, instead of computing an averaged prediction from the k nearest neighbors, our prediction <i>will</i> be the k nearest neighbors. Furthermore, <i>k = n</i> (where n is one of the input parameters).</p>
            <p>In the future, possible improvements include creating a weighted k-NN model. The songs could be weighted by popularity/occurrence in the training set, how close they are to the original song, or by some other feature we wish to include. </p>

            <h5>k-Means Clustering</h5>
            <p>k-Means Clustering is unsupervised machine learning algorithm which aims to split n observations (songs) into k clusters based on the nearest mean. k-Means is an NP-hard problem which means that it is computationally difficult to find the best answer (“K-Means Clustering”). Problems in NP are believed to not be solvable in polynomial time, however, this has not been proven (“Rocchio Algorithm”). However, efficient algorithms which converge to a local optimum exist.</p>

            <p>k-Means can work well even when data is not on the same scale. To be consistent with our k-NN based models, we will construct a set of k-Means models which include vs. omit tempo from the training data. </p>
            <p>Since we want our model to predict a playlist, we will have to slightly modify our k-Means model. To do so, we will construct a k-Means model, then we will predict the cluster the seed song belongs to. This cluster will become the selected cluster. A random sample of <i>n</i> songs will be drawn from the cluster and will create the predicted playlist. We randomly select the songs from the predicted cluster so as to better test the clustering functionality of the k-Means model.  </p>
            <p>In the future, we could implement a smarter selection criteria for selecting songs within a cluster. A smarter choice would be to implement a k-NN model within a cluster to select the <i>n</i> songs. This variation is known as the Rocchio algorithm (“Rocchio Algorithm”). We did not want to test this variation because it blurs the line between k-NN and k-Means which we wish to keep clear for the sake of our model comparisons.</p>
            <h5>k-Nearest Neighbors vs. k-Means Clustering</h5>
            <p>There are a few clear differences between the k-NN and k-Means algorithms. First, k-NN solves a classification or regression problem while k-Means solves a clustering problem. Second, as explained above, k-NN is a lazy learner while k-Means is an eager learner. A lazy learner saves computation until absolutely needed as we observe in k-NN. An eager learner has a training step which creates and fits a model. Finally, as discussed above, k-NN functions much better if the data is all on the same scale while k-Means does not have this same constraint (Dhiraj).</p>
            <p>Comparing these two sets of models allows us to compare a human tailored selection model (k-NN based) to a computer selection model (k-Means Clustering based). We will use this comparison as a testing metric discussed in the Results section. </p>

            <h3>Final Models</h3>
            <p>Our k-NN models will have two variantes: distance metric and tempo. This leads to four k-NN models total. The four models are:</p>
            <ol>
                <li>k-NN using <b>Euclidean distance</b> and <b>raw</b> tempo</li>
            <li>k-NN using <b>Euclidean distance</b> and <b>no</b> tempo</li>
            <li>k-NN using <b>cosine similarity</b> and <b>raw</b> tempo</li>
            <li>k-NN using <b>cosine similarity</b> and <b>no</b> tempo</li>
            </ol>

            <p>We decided to omit comparison to normalized tempo after testing normalized tempo and seeing that the playlists constructed with tempo normalized typically consist of songs missing the audio features we aim to test. Missing these audio features breaks our testing metric making it hard to compare to the other models. For this reason, we decided to omit normalization of tempo.</p>

            <p>Here is our general k-NN model:</p>
            <img class="center-img" src="images/knn_model_img.png" width="85%">
            <br>
            <p>Our general k-NN model is used to construct each of the four models. The inputs are the dataframe of songs to train the model on, the seed song to create a playlist from, the length of the desired playlist, the desired distance metric, and a boolean representing whether or not to include tempo in the model.</p>

            <p>We first define the desired audio feature columns with and without tempo. Then we convert the seed song to a dataframe of it’s audio features. If the distance metric is cosine similarity, we compare the songs dataframe to the seed song using cosine similarity. Then, we sort the values in descending order because a higher cosine similarity means that the song at hand is more similar to the seed song. If the distance metric is euclidean distance, we compare the songs dataframe to the seed song using euclidean distance. Then, we sort the values in ascending order because a smaller euclidean distance means that the song at hand is closer to the seed song. In both cases, we take the top n best values. We then extract and return the index of these values. From these indices, we can find the corresponding song by matching the index in the songs dataframe.</p>

            <p>Using the indices returned from the k-NN model, we print out the playlist as follows:</p>
            <img class="center-img" src="images/playlist_printer.png" width="85%">
            <br>
            <p>Looping through the indices, we locate each song in the songs dataframe. Then, we use the track uri to access the Spotify API and extract human readable information which we print to the screen. This includes the track name and track artists. Simultaneously, we append the track’s audio features to a dataframe so we also have a numerical view of the playlist which will be used in testing.</p>

            <p>Now, we use the k-NN model and the playlist printer to define the four k-NN models:</p>
            <img class="center-img" src="images/knn_models.png" width="85%">
            <br>
            <p>In order to quickly run and analyze all four models, we create a function which runs all of the models and creates a dataframe of the mean audio feature values for each predicted playlist.</p>
            <img class="center-img" src="images/run_all_knn.png" width="85%">
            <br>
            <p>We also add the seed song’s audio features to the mean dataframe to have a comparison value. Furthermore, we fill NaN values in the mean dataframe with 0 to maintain uniformity with Spotify’s functionality (Spotify returns 0 for audio features that are not recorded).</p>

            <p>Sample output from <code>run_all_knn</code> using Michael Bublé’s <i>It’s Beginning to Look a lot Like Christmas</i> as the seed song is as follows:</p>

            <div class="center-frame">
                <iframe class="spotify-song" src="https://open.spotify.com/embed/track/5a1iz510sv2W9Dt1MvFd5R" width="300" height="80" frameborder="0" allowtransparency="true" allow="encrypted-media"></iframe>
            </div>

            <div class="two-imgs">
                <img class="center-img" src="images/playlist_knn_et.png" width="24%">
                <img class="center-img" src="images/playlist_knn_ent.png" width="24%">
                <img class="center-img" src="images/playlist_knn_ct.png" width="24%">
                <img class="center-img" src="images/playlist_knn_cnt.png" width="24%">
            </div>

            <div class="two-imgs">
                <iframe class="center-img" src="https://open.spotify.com/embed/playlist/6q0RLVuPRJpYDoYQ3hMWpC?si=elLzU2HcSzulCwtOgs8Qyg" width="24%" height="300" frameborder="0" allowtransparency="true" allow="encrypted-media"></iframe>
                <iframe class="center-img" src="https://open.spotify.com/embed/playlist/7qjXPO8GmAYBBF40yxbIR6?si=X67AdgnYT7WCoh9XXnRu6Q" width="24%" height="300" frameborder="0" allowtransparency="true" allow="encrypted-media"></iframe>
                <iframe class="center-img" src="https://open.spotify.com/embed/playlist/57G0IpR7PrayUWwy9fCCXO?si=y8lm-vJUSranqYcI-bEwaQ" width="24%" height="300" frameborder="0" allowtransparency="true" allow="encrypted-media"></iframe>
                <iframe class="center-img" src="https://open.spotify.com/embed/playlist/3xIwbMFpDPze2x6Knt6u1c?si=bCVwuSUqSvqiT85VKCgasg" width="24%" height="300" frameborder="0" allowtransparency="true" allow="encrypted-media"></iframe>
            </div>
            <br>
            <img class="center-img" src="images/knn_output.png" width="70%">
            <br>

            <p>Next, we have our k-Means Clustering model. The set up is similar to the k-NN model. We first have a general k-Means model. Then, we use this function to construct the two separate models. Finally, we have a function which runs both models, prints the predicted playlists, and returns a means dataframe for analysis and testing purposes.</p>

            <p>Here is our general k-Means Clustering model:</p>
            <img class="center-img" src="images/kmeans_model_img.png" width="85%">
            <br>

            <p>Our general k-Means Clustering model is used to construct both k-Means models. The inputs are the number <i>k</i> of clusters, the dataframe of songs to train the model on, the seed song to create a playlist from, the length of the desired playlist, a boolean representing whether or not to include tempo in the model, and the k-Means model if it has previously been generated. Differently from k-NN, we only need to train the model once because k-Means Clustering is an eager learner.</p>

            <p>We first define the dataframe with the desired audio feature columns with and without tempo. If a k-Means model has already been fit, then we do not train another model. Otherwise, we must train and fit a k-Means model. Then, we use the k-Means model to predict the clusters for all of the songs. We convert the seed song to a dataframe of it’s audio features and predict the song’s corresponding cluster. Finally, we randomly select <i>n</i> songs from the seed song’s predicted cluster. We then extract and return the index of these values. From these indices, we can find the corresponding song by matching the index in the songs dataframe. We use the playlist printer defined above for the k-Means models as well.</p>

            <p>Now, we use the k-Means Clustering model and the playlist printer to define the two k-Means models:</p>
            <img class="center-img" src="images/kmeans_models.png" width="85%">
            <br>

            <p>To store and reuse the k-Means models, we create two global variables. If we want to refit the models, then we call the <code>reset_models</code> function:</p>
            <img class="center-img" src="images/reset_models.png" width="85%">
            <br>

            <p>In order to quickly run and analyze all four models, we create a function which runs all of the models and creates a dataframe of the mean audio feature values for each predicted playlist.</p>
            <img class="center-img" src="images/run_all_kmeans.png" width="85%">
            <br>

            <p>Sample output from <code>run_all_kmeans</code> using Michael Bublé’s <i>It’s Beginning to Look a lot Like Christmas</i> as the seed song is as follows:</p>

            <div class="center-frame">
                <iframe class="spotify-song" src="https://open.spotify.com/embed/track/5a1iz510sv2W9Dt1MvFd5R" width="300" height="80" frameborder="0" allowtransparency="true" allow="encrypted-media"></iframe>
            </div>

            <div class="two-imgs">
                <img class="center-img" src="images/playlist_kmeans_t.png" width="40%">
                <img class="center-img" src="images/playlist_kmeans_nt.png" width="40%">
            </div>

            <div class="two-imgs">
                <iframe class="center-img" src="https://open.spotify.com/embed/playlist/018Fpy0XFu5yklnT094LCb?si=FBH4_04LR0-JTx1WkF5VFQ" width="24%" height="300" frameborder="0" allowtransparency="true" allow="encrypted-media"></iframe>
                <iframe class="center-img" src="https://open.spotify.com/embed/playlist/6wyWoCDRJldSZdsRo2ZMj6?si=9gSZWoAtTp6s2WJwuX-7xw" width="24%" height="300" frameborder="0" allowtransparency="true" allow="encrypted-media"></iframe>
            </div>

            <br>
            <img class="center-img" src="images/kmeans_output.png" width="70%">
            <br>

            <p>In the next section, we will test and explore the results produced from using these six models.</p>
            <br>

            <h5>Works Cited</h5>
              <ol>
                 <li>
                    <p>K, Dhiraj. “Difference between K-Nearest Neighbor(K-NN) and K-Means Clustering.” Medium, Medium, 29 Aug. 2019, <br>medium.com/@dhiraj8899/difference-between-k-nearest-neighbor-k-nn-and-k-means-clustering-d9a44859182f.</p>
                 </li>
                 <li>
                    <p>“K-Means Clustering.” Wikipedia, Wikimedia Foundation, 3 Dec. 2019, <br> en.wikipedia.org/wiki/K-means_clustering.</p>
                 </li>
                 <li>
                    <p>“Machine Learning Basics with the K-Nearest Neighbors Algorithm.” Accessed December 9, 2019. <br>https://towardsdatascience.com/machine-learning-basics-with-the-k-nearest-neighbors-algorithm-6a6e71d01761.</p>
                 </li>
                 <li>
                     <p>“NP-Hardness.” Wikipedia, Wikimedia Foundation, 20 Nov. 2019, <br>https://en.wikipedia.org/wiki/NP-hardness.</p>
                 </li>
                 <li>
                     <p>“Rocchio Algorithm.” Wikipedia, Wikimedia Foundation, 3 Nov. 2019, <br>https://en.wikipedia.org/wiki/Rocchio_algorithm.</p>
                 </li>
              </ol>
        </div>

        <div class="section pp-scrollable">
            <h1>Results: Evaluating our playlists</h1>
            <p>Once we had created our playlists using our k-NN and k-Means models, we created two main testing methods by which to evaluate the quality of our playlists. These methods and their results are described in this section of our report. </p>
            <br/>
            <h3>Method 1: Cluster Evaluation</h3>
            <p>The first  </p>
            <h3>Method 2: User-Created Playlist Audio Features</h3>

        </div>

        <div class="section">
            <h1>Conclusions</h1>
            <p></p>
        </div>
        <div class="section">
            <h1>Full Bibliography</h1>
		      <ol>
                <li>
                    <p>K, Dhiraj. “Difference between K-Nearest Neighbor(K-NN) and K-Means Clustering.” Medium, Medium, 29 Aug. 2019, <br>medium.com/@dhiraj8899/difference-between-k-nearest-neighbor-k-nn-and-k-means-clustering-d9a44859182f.</p>
                 </li>
                <li>
                     <p>Dieleman, Sander. “Recommending Music on Spotify with Deep Learning.” Accessed December 9, 2019. <br>https://benanne.github.io/2014/08/05/spotify-cnns.html.</p>
                 </li>
                <li>
                    <p>Giacaglia, Giuliano. “Behind Spotify Recommendation Engine.” Medium, May 21, 2019. <br> https://medium.com/datadriveninvestor/behind-spotify-recommendation-engine-a9b5a27a935.</p>
                 </li>
                <li>
                    <p>Johnson, Chris. “Algorithmic Music Recommendations at Spotify.” Technology, 11:33:57 UTC. <br> https://www.slideshare.net/MrChrisJohnson/algorithmic-music-recommendations-at-spotify/3-What_is_SpotifyOn_demand_music.</p>
                 </li>
		         <li>
		            <p>Porter, “Spotify Is First to 100 Million Paid Subscribers.”</p>
		         </li>
		         <li>
		            <p>“Discover Weekly: How Spotify Is Changing the Way We Consume Music – Technology and Operations Management.”</p>
		         </li>

                 <li>
                    <p>“K-Means Clustering.” Wikipedia, Wikimedia Foundation, 3 Dec. 2019, <br> en.wikipedia.org/wiki/K-means_clustering.</p>
                 </li>
                 <li>
                    <p>“Machine Learning Basics with the K-Nearest Neighbors Algorithm.” Accessed December 9, 2019. <br>https://towardsdatascience.com/machine-learning-basics-with-the-k-nearest-neighbors-algorithm-6a6e71d01761.</p>
                 </li>
                 <li>
                     <p>“NP-Hardness.” Wikipedia, Wikimedia Foundation, 20 Nov. 2019, <br>https://en.wikipedia.org/wiki/NP-hardness.</p>
                 </li>
                 <li>
                     <p>“Rocchio Algorithm.” Wikipedia, Wikimedia Foundation, 3 Nov. 2019, <br>https://en.wikipedia.org/wiki/Rocchio_algorithm.</p>
                 </li>
		      </ol>
        </div>
    </div>

    <script src="js/jquery.min.js"></script>
    <script src="js/bootstrap.min.js"></script>
<!--    <script src="http://ajax.googleapis.com/ajax/libs/jquery/1.11.1/jquery.min.js"></script>-->
    <script src="js/jquery.pagepiling.min.js"></script>
    <script src="js/d3.min.js"></script>
    <script src="js/main.js"></script>
</body>
</html>
